{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Post Body and Post Title\n",
    "\n",
    "This was an attempt to bucket post body and post title topics.  Unfortunately, I was unable to come up with any meaningful topics or identify significant differences between the r/Depression and r/SuicideWatch subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)][0]\n",
    "\n",
    "def preprocess(doc):\n",
    "    result = []\n",
    "    for word in doc:\n",
    "        for token in gensim.utils.simple_preprocess(word):\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "                result.append(lemmatize_text(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(934, 41)\n",
      "(980, 41)\n"
     ]
    }
   ],
   "source": [
    "df_d_c = pd.read_csv('depression_comments.csv').drop('Unnamed: 0', axis = 1)\n",
    "df_s_c = pd.read_csv('suicidewatch_comments.csv').drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "df_d_c = df_d_c.drop_duplicates(['p_id'], keep='last')\n",
    "df_s_c = df_s_c.drop_duplicates(['p_id'], keep='last')\n",
    "\n",
    "print(df_d_c.shape)\n",
    "print(df_s_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "newStopWords = ['feel','want', 'dont', 'think', 'need', 'feeling']\n",
    "sw.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469                             [regular, checkin, post]\n",
       "517    [mostbroken, leastunderstood, rule, helper, in...\n",
       "554              [depressed, people, way, nicer, people]\n",
       "617                                        [browse, sub]\n",
       "629                                       [fix, problem]\n",
       "637    [head, depression, isnt, real, excuse, one, go...\n",
       "639    [purposely, sad, miserable, loop, selffailure,...\n",
       "660                     [actually, going, school, today]\n",
       "678                  [love, wearing, oversized, hoodies]\n",
       "696                                  [life, short, long]\n",
       "Name: title_pre, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_d_c['title_sw_p'] = df_d_c['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)])).astype(str).str.replace('[^\\w\\s]', '').str.lower()\n",
    "df_d_c['title_pre'] = df_d_c['title_sw_p'].str.split(' ')\n",
    "processed_docs = df_d_c['title_pre'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 checkin\n",
      "1 post\n",
      "2 regular\n",
      "3 contact\n",
      "4 explain\n",
      "5 helper\n",
      "6 invite\n",
      "7 leastunderstood\n",
      "8 mostbroken\n",
      "9 new\n",
      "10 private\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.183*\"im\" + 0.088*\"help\" + 0.078*\"life\" + 0.067*\"depressed\" + 0.058*\"day\" + 0.052*\"time\" + 0.051*\"like\" + 0.047*\"sad\" + 0.045*\"today\" + 0.044*\"tired\"\n",
      "Topic: 1 \n",
      "Words: 0.127*\"depression\" + 0.111*\"like\" + 0.110*\"im\" + 0.076*\"people\" + 0.068*\"life\" + 0.066*\"know\" + 0.059*\"dont\" + 0.054*\"hate\" + 0.046*\"die\" + 0.042*\"going\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=2, \n",
    "                                       id2word=dictionary, passes=2, workers=2)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.142*\"depression\" + 0.100*\"help\" + 0.079*\"people\" + 0.071*\"im\" + 0.064*\"know\" + 0.055*\"sad\" + 0.053*\"anymore\" + 0.047*\"time\" + 0.046*\"thing\" + 0.046*\"today\"\n",
      "Topic: 1 Word: 0.160*\"im\" + 0.117*\"life\" + 0.110*\"like\" + 0.064*\"depressed\" + 0.048*\"hate\" + 0.045*\"day\" + 0.045*\"dont\" + 0.044*\"friend\" + 0.042*\"going\" + 0.040*\"feeling\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=2, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131    [new, wiki, avoid, accidentally, encouraging, ...\n",
       "171    [reminder, absolutely, activism, kind, allowed...\n",
       "192     [store, ice, cream, kill, idk, ill, decide, car]\n",
       "219                                             [failed]\n",
       "247        [fear, whats, keep, killing, make, depressed]\n",
       "258                                                [day]\n",
       "303                                  [absurd, like, wtf]\n",
       "321                    [idea, reincarnation, terrifying]\n",
       "328    [update, told, mom, time, tried, commit, suicide]\n",
       "336                                 [anonymous, goodbye]\n",
       "Name: title_pre, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s_c['title_sw_p'] = df_s_c['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)])).astype(str).str.replace('[^\\w\\s]', '').str.lower()\n",
    "df_s_c['title_pre'] = df_s_c['title_sw_p'].str.split(' ')\n",
    "processed_docs = df_s_c['title_pre'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 accidentally\n",
      "1 avoid\n",
      "2 covert\n",
      "3 encouraging\n",
      "4 incitement\n",
      "5 new\n",
      "6 spot\n",
      "7 suicide\n",
      "8 wiki\n",
      "9 absolutely\n",
      "10 activism\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.130*\"im\" + 0.109*\"life\" + 0.076*\"suicide\" + 0.071*\"die\" + 0.070*\"like\" + 0.047*\"help\" + 0.045*\"end\" + 0.044*\"going\" + 0.040*\"time\" + 0.037*\"friend\"\n",
      "Topic: 1 \n",
      "Words: 0.168*\"im\" + 0.084*\"kill\" + 0.055*\"dont\" + 0.051*\"people\" + 0.048*\"anymore\" + 0.046*\"suicidal\" + 0.044*\"thought\" + 0.044*\"today\" + 0.039*\"suicide\" + 0.033*\"live\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=2, \n",
    "                                       id2word=dictionary, passes=2, workers=2)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.099*\"suicide\" + 0.098*\"life\" + 0.085*\"die\" + 0.062*\"like\" + 0.056*\"help\" + 0.055*\"im\" + 0.049*\"end\" + 0.037*\"fucking\" + 0.034*\"day\" + 0.033*\"time\"\n",
      "Topic: 1 Word: 0.205*\"im\" + 0.082*\"kill\" + 0.052*\"going\" + 0.051*\"anymore\" + 0.050*\"dont\" + 0.047*\"know\" + 0.045*\"people\" + 0.044*\"today\" + 0.042*\"suicidal\" + 0.038*\"help\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=2, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 469                                 Regular Check-In Post\n",
       "517     Our most-broken and least-understood rules is ...\n",
       "554     is it just me or are depressed people WAY nice...\n",
       "617              Anyone else just browse this sub and cry\n",
       "629              I could fix all my problems, yet I don't\n",
       "                              ...                        \n",
       "4030                       Therapists have feelings, too!\n",
       "4031                    Dexamphetamine for lack of energy\n",
       "4032                                  Back at square one.\n",
       "4033                                        what do i do?\n",
       "4034                                           Depression\n",
       "Name: title, Length: 934, dtype: object>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_d_c['title'].unique"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
